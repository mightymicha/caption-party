{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Subtitle Analysis\n",
    "Here we analyze subtitles fetched from YouTube.\n",
    "\n",
    "## Imports\n",
    "Import matplotlib package and configure some plot drawing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 54109,\n",
      "  \"iopub_port\": 53049,\n",
      "  \"stdin_port\": 53097,\n",
      "  \"control_port\": 40287,\n",
      "  \"hb_port\": 51901,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"e1d13183-6b2f3928f75f369586908789\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-0f81b87b-e661-40b9-bc76-3bdba3374492.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'png')\n",
    "plt.rcParams['savefig.dpi'] = 75\n",
    "\n",
    "plt.rcParams['figure.autolayout'] = False\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 13\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.serif'] = \"cm\"\n",
    "plt.rcParams['text.latex.preamble'] = r\"\\usepackage{type1cm}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import isodate\n",
    "import pytz\n",
    "\n",
    "conn = sqlite3.connect(\"../rsc/caption_party.db\")\n",
    "data = pd.read_sql(\"SELECT * from tab\", conn)\n",
    "\n",
    "cest = pytz.timezone(\"Europe/Berlin\")\n",
    "\n",
    "data['publishedAt'] = pd.to_datetime(data['publishedAt']).dt.tz_convert(cest)\n",
    "data['updated'] = pd.to_datetime(data['updated']).dt.tz_convert(cest)\n",
    "\n",
    "data['viewCount'] = pd.to_numeric(data['viewCount'], downcast='unsigned')\n",
    "data['commentCount'] = pd.to_numeric(data['commentCount'], downcast='unsigned', errors='coerce')\n",
    "\n",
    "data['tags'] = data['tags'].apply(lambda t: t.split(\"'\")[1::2])\n",
    "data['duration'] = data['duration'].apply(lambda t: isodate.parse_duration(t).total_seconds())\n",
    "\n",
    "no_subs_ids = data['subtitle'].apply(lambda x: len(x) <= 10)\n",
    "data.loc[no_subs_ids, 'subtitle'] = data.loc[no_subs_ids, 'description']\n",
    "\n",
    "data.set_index('videoId', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "### Dates and periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as time, timedelta\n",
    "\n",
    "bt_election = cest.localize(time.fromisoformat('2017-09-24')).replace(hour=8)\n",
    "eu_election = cest.localize(time.fromisoformat('2019-05-26')).replace(hour=8)\n",
    "\n",
    "bt_start = bt_election - timedelta(days=83)\n",
    "eu_start = eu_election - timedelta(days=83)\n",
    "bt_end = bt_election + timedelta(days=84)\n",
    "eu_end = eu_election + timedelta(days=84)\n",
    "\n",
    "bt_period = (bt_start <= data['publishedAt']) & (data['publishedAt'] <= bt_election)\n",
    "eu_period = (eu_start <= data['publishedAt']) & (data['publishedAt'] <= eu_election)\n",
    "bt_campaign_period = (bt_start <= data['publishedAt']) & (data['publishedAt'] <= bt_election)\n",
    "eu_campaign_period = (eu_start <= data['publishedAt']) & (data['publishedAt'] <= eu_election)\n",
    "\n",
    "corpus = data[bt_period|eu_period]\n",
    "bt_corpus = data[bt_period]\n",
    "eu_corpus = data[eu_period]\n",
    "\n",
    "campaign_corpus = data[bt_campaign_period|eu_campaign_period]\n",
    "bt_campaign_corpus = data[bt_campaign_period]\n",
    "eu_campaign_corpus = data[eu_campaign_period]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parties, elections and states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = {'union': 'Union',\n",
    "         'spd': 'SPD',\n",
    "         'afd': 'AfD',\n",
    "         'fdp': 'FDP',\n",
    "         'linke': 'Die Linke',\n",
    "         'grüne': 'Die Grünen'}\n",
    "elections = ['bt', 'eu']\n",
    "new_states = ['BB', 'MV', 'SN', 'ST', 'TH']\n",
    "old_states = ['BW', 'BY', 'HB', 'HH', 'HE', 'NI', 'NW', 'RP', 'SL', 'SH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mc\n",
    "import colorsys\n",
    "\n",
    "def scale_color(color, amount=0.5):\n",
    "    c_hls = colorsys.rgb_to_hls(*mc.to_rgb(color))\n",
    "    c_rgb = colorsys.hls_to_rgb(c_hls[0], 1 - amount * (1 - c_hls[1]), c_hls[2])\n",
    "    return mc.to_hex(c_rgb)\n",
    "\n",
    "colors = {\n",
    "    'union': '#252422',\n",
    "    'spd':   '#e2001a',\n",
    "    'afd':   '#009ee0',\n",
    "    'fdp':   '#ffec01',\n",
    "    'linke': '#ffa7b6',\n",
    "    'grüne': '#42923b'}\n",
    "\n",
    "colors_light = {key: scale_color(color, 0.35) for key, color in colors.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and tokenization\n",
    "\n",
    "2. Replaces `<letter>-<letter>` by `<letter><letter>`\n",
    "2. Replaces `<letter>*<letter>` by `<letter><letter>`\n",
    "3. Replaces `<non-words>-<newline><non-words>` by nothing\n",
    "4. Replaces `<non-words><newline><non-words>` by single space\n",
    "5. Replaces `<non-words>` by single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_tokenize(text, remove_numbers=False):\n",
    "    if remove_numbers:\n",
    "        replaced = re.sub(r'\\d+', ' ', text)\n",
    "    else:\n",
    "        replaced = text\n",
    "    replaced = re.sub(r'(\\w)-(\\w)', r'\\1\\2', replaced)\n",
    "    replaced = re.sub(r'(\\w)\\*(\\w)', r'\\1\\2', replaced)\n",
    "    replaced = re.sub(r'\\W*-\\\\n\\W*', '', replaced)\n",
    "    replaced = re.sub(r'\\W*\\\\n\\W*', ' ', replaced)\n",
    "    replaced = re.sub(r'\\W+', ' ', replaced)\n",
    "    return replaced.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('german'))\n",
    "\n",
    "def test_stop_word(word):\n",
    "    return (word.lower() in stop) or (word.capitalize() in stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enchant import Dict\n",
    "\n",
    "dic = Dict(\"de_DE\")\n",
    "\n",
    "def test_dictionary(word):\n",
    "    return dic.check(word.lower()) or dic.check(word.capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de import lemmatizer\n",
    "\n",
    "def lemmatize_spacy(word):\n",
    "    try:\n",
    "        return lemmatizer.LOOKUP[word]\n",
    "    except: pass\n",
    "    \n",
    "    # Try to lemmatize lower word version\n",
    "    try:\n",
    "        return lemmatizer.LOOKUP[word.lower()]\n",
    "    except: pass\n",
    "    \n",
    "    # Try to lemmatize capitalized word version\n",
    "    try:\n",
    "        return lemmatizer.LOOKUP[word.capitalize()]\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treetagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/treetaggerwrapper.py:740: FutureWarning: Possible nested set at position 8\n",
      "  re.IGNORECASE | re.VERBOSE)\n",
      "/usr/lib/python3.7/site-packages/treetaggerwrapper.py:2044: FutureWarning: Possible nested set at position 152\n",
      "  re.VERBOSE | re.IGNORECASE)\n",
      "/usr/lib/python3.7/site-packages/treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "/usr/lib/python3.7/site-packages/treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "from treetaggerwrapper import TreeTagger\n",
    "\n",
    "tagger = TreeTagger(TAGLANG='de')\n",
    "\n",
    "def lemmatize_tt(word):\n",
    "    \n",
    "    def lemmatize(w):\n",
    "        tt_lower = tagger.tag_text([w], tagonly=True)[0].split('\\t')[-1]\n",
    "        if tt_lower != w:\n",
    "            return tt_lower.split(\"|\")[-1]\n",
    "    \n",
    "    lem = lemmatize(word)\n",
    "    if lem is not None:\n",
    "        return lem\n",
    "    \n",
    "    # Try to lemmatize lower word version\n",
    "    lem = lemmatize(word.lower())\n",
    "    if lem is not None:\n",
    "        return lem\n",
    "    \n",
    "    # Try to lemmatize capitalized word version\n",
    "    lem = lemmatize(word.capitalize())\n",
    "    if lem is not None:\n",
    "        return lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer combiner\n",
    "- If one lemmatizer exclusively modifies the word, it returns this lemmatized version\n",
    "- If both lemmatizer modify the word, it returns the spaCy version\n",
    "- If neither one modifies the word, it returns just the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_combined(word):\n",
    "    sp = lemmatize_spacy(word)\n",
    "    tt = lemmatize_tt(word)\n",
    "    if sp is not None:\n",
    "        return sp\n",
    "    elif tt is not None:\n",
    "        return tt\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "from itertools import chain\n",
    "from IPython.display import display_html\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_bar(data, label):\n",
    "    \"\"\" Returns a bar chart in which a DataFrame is visualized.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(color='gray', linestyle='dashed', alpha=0.3, axis='y')\n",
    "    ax.set_ylabel(label);\n",
    "    color_map = [colors[party] for party in parties]\n",
    "    ax.bar(parties, channel_count[parties], color=color_map, width=.8)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_compare_bar(bt_data, eu_data, label, left=False):\n",
    "    \"\"\" Returns a bar chart where two DataFrames are compared.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    ax.set_ylabel(label);\n",
    "    ax.grid(color='gray', linestyle='dashed', alpha=0.3, axis='y')\n",
    "    color_map_dark = [colors[party] for party in parties]\n",
    "    color_map_light = [colors_light[party] for party in parties]\n",
    "    ax.bar(parties.values(), bt_data[parties.keys()], color=color_map_dark, width=-.4, align='edge')\n",
    "    ax.bar(parties.values(), eu_data[parties.keys()], color=color_map_light, width=.4, align='edge')\n",
    "    print_legend(ax, left)\n",
    "    return fig\n",
    "\n",
    "def generate_wordcloud(weight_matrix, title):\n",
    "    \"\"\" The following function takes a weight matrix as an input with parties as columns and words as indices.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(18, 15))\n",
    "    plt.suptitle(title, size=25, weight='bold')\n",
    "    axis = chain.from_iterable(zip(*axs))\n",
    "    for party, ax in zip(weight_matrix.columns, axis):\n",
    "        # Visuals\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        ax.set_title(party.upper(), size=20)\n",
    "        color_func = lambda *args, **kwargs: colors[party]\n",
    "        # Weights and Clourd\n",
    "        word_weights = top_n(weight_matrix, party, 30).iloc[:,0].to_dict()\n",
    "        word_cloud = WordCloud(background_color='white', height=500, width=1000, color_func=color_func)\n",
    "        image = word_cloud.generate_from_frequencies(word_weights)\n",
    "        ax.imshow(image, interpolation='bilinear')\n",
    "    return fig\n",
    "\n",
    "        \n",
    "def display_side_by_side(*args):\n",
    "    \"\"\" Displays mutiple panda DataFrames side by side.\n",
    "    \"\"\"\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "    \n",
    "    \n",
    "def top_n(df, column, n):\n",
    "    \"\"\" Sorts a pandas DataFrame and returns the first n entries of a specific column.\n",
    "    \"\"\"\n",
    "    return df[[column]].sort_values(ascending=False, by=column).iloc[:n]\n",
    "\n",
    "\n",
    "def print_legend(ax, left=False):\n",
    "    if left:\n",
    "        label = ax.annotate('Bundestag election\\n European election', xy=(0, 1), xytext=(57, -10),\n",
    "                        fontsize=13, horizontalalignment='left', verticalalignment='top',\n",
    "                        xycoords='axes fraction', textcoords='offset points',)\n",
    "    else:\n",
    "        label = ax.annotate('Bundestag election\\n European election', xy=(1, 1), xytext=(-15, -10),\n",
    "                        fontsize=13, horizontalalignment='right', verticalalignment='top', \n",
    "                        xycoords='axes fraction', textcoords='offset points')\n",
    "    ax.figure.canvas.draw()\n",
    "    bbox = label.get_window_extent()\n",
    "    bbox_data = ax.transData.inverted().transform(bbox) \n",
    "    text_x_left = bbox_data[0][0]\n",
    "    text_x_right = bbox_data[1][0]\n",
    "    text_y_bottom = bbox_data[0][1]\n",
    "    text_y_top = bbox_data[1][1]\n",
    "\n",
    "    text_width = text_x_right - text_x_left\n",
    "    text_height = text_y_top - text_y_bottom\n",
    "\n",
    "    patches_width = 0.4 * text_width\n",
    "    x_box_border = 0.04 * text_width\n",
    "    y_box_border = 0.04 * text_height\n",
    "\n",
    "    box_width = patches_width + text_width + 2 * x_box_border\n",
    "    box_height = text_height + 5 * y_box_border\n",
    "\n",
    "    box_position = (text_x_left-patches_width-x_box_border, text_y_bottom - 2 * y_box_border)\n",
    "\n",
    "    box = mpatches.Rectangle(box_position, box_width, box_height, alpha=.2, facecolor='white', edgecolor='black')\n",
    "    ax.add_patch(box)\n",
    "\n",
    "    num_patches = len(parties)\n",
    "    patch_width = (patches_width - 2 * x_box_border)/num_patches\n",
    "    patch_height = (box_height/2 - 4.5 * y_box_border)\n",
    "\n",
    "    for num, party in enumerate(parties):\n",
    "        patch_x = box_position[0] + x_box_border + num * patch_width\n",
    "        patch_y = box_position[1] + 2.5 * y_box_border\n",
    "        patch = mpatches.Rectangle((patch_x, patch_y), patch_width, patch_height, color=colors_light[party])\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "        patch_x = box_position[0] + x_box_border + num * patch_width\n",
    "        patch_y = box_position[1] + box_height - patch_height - 2.5 * y_box_border\n",
    "        patch = mpatches.Rectangle((patch_x, patch_y), patch_width, patch_height, color=colors[party])\n",
    "        ax.add_patch(patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yt_text(party, election, campaign):\n",
    "    \"\"\" Returns a data slice containing videos from a specific party and electoral campaign period\n",
    "    \"\"\"\n",
    "    party_vec = (corpus['party'] == party)\n",
    "    if election == 'bt' and not campaign:\n",
    "        return bt_corpus.loc[party_vec, 'subtitle']\n",
    "    elif election == 'bt' and campaign:\n",
    "        return bt_campaign_corpus.loc[party_vec, 'subtitle']\n",
    "    elif election == 'eu' and not campaign:\n",
    "        return eu_corpus.loc[party_vec, 'subtitle']\n",
    "    elif election == 'eu' and campaign:\n",
    "        return eu_campaign_corpus.loc[party_vec, 'subtitle']\n",
    "    else: \n",
    "        return corpus.loc[party_vec, 'subtitle']\n",
    "                \n",
    "        \n",
    "def preprocess(raw_text,\n",
    "               filters=[],\n",
    "               lemmatize=True,\n",
    "               lowercase=True,\n",
    "               min_chars=0,\n",
    "               remove_numbers=False):\n",
    "    \"\"\" Traversed some string through a preprocessing pipeline: \n",
    "    min chars, stop word removal, optional filters, lemmatization and lowercasing\n",
    "    \"\"\"\n",
    "        \n",
    "    # Cleaning & tokenization\n",
    "    tokens = clean_and_tokenize(raw_text, remove_numbers=remove_numbers)\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        # Min char removal\n",
    "        if len(token) < min_chars:\n",
    "            continue\n",
    "            \n",
    "        # Stop word removal\n",
    "        if test_stop_word(token):\n",
    "            continue\n",
    "            \n",
    "        # Additional filters \n",
    "        if not all(f(token) for f in filters):\n",
    "            continue \n",
    "            \n",
    "        # Lemmatization\n",
    "        lem_token = token\n",
    "        if lemmatize:\n",
    "            lem_token = lemmatize_combined(token)\n",
    "        \n",
    "        # Lowercasing\n",
    "        if lowercase:\n",
    "            lem_token = lem_token.lower()\n",
    "            \n",
    "        filtered_tokens.append(lem_token)\n",
    "        \n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def get_subs(party, election, merge=False, campaign=False):\n",
    "    video_subs = get_yt_text(party, election, campaign)\n",
    "    processed_subs = []\n",
    "    for sub in video_subs:\n",
    "        lemma_list = preprocess(sub, min_chars=4)\n",
    "        processed_subs.append(lemma_list)\n",
    "    if merge:\n",
    "        return [lemma \n",
    "                for lemma_list in processed_subs\n",
    "                for lemma in lemma_list]\n",
    "    else:\n",
    "        return processed_subs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "### Get corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-427936ba5f9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msubs_bt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparties\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msubs_eu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparties\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcampaign_subs_bt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcampaign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparties\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcampaign_subs_eu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcampaign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparties\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-427936ba5f9f>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msubs_bt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparties\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msubs_eu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparties\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcampaign_subs_bt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcampaign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparties\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcampaign_subs_eu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcampaign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparties\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d018e7cc7afc>\u001b[0m in \u001b[0;36mget_subs\u001b[0;34m(party, election, merge, campaign)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprocessed_subs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvideo_subs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mlemma_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mprocessed_subs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d018e7cc7afc>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(raw_text, filters, lemmatize, lowercase, min_chars, remove_numbers)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mlem_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mlem_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_combined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Lowercasing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5977d50b2201>\u001b[0m in \u001b[0;36mlemmatize_combined\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize_combined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_tt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bdfd3c7236ba>\u001b[0m in \u001b[0;36mlemmatize_tt\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Try to lemmatize lower word version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mlem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bdfd3c7236ba>\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtt_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtt_lower\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtt_lower\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/treetaggerwrapper.py\u001b[0m in \u001b[0;36mtag_text\u001b[0;34m(self, text, numlines, tagonly, prepronly, tagblanks, notagurl, notagemail, notagip, notagdns, nosgmlsplit)\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0mlastline_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagoutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read from TreeTagger: %r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subs_bt = {party: get_subs(party, 'bt', merge=True) for party in parties}\n",
    "subs_eu = {party: get_subs(party, 'eu', merge=True) for party in parties}\n",
    "campaign_subs_bt = {party: get_subs(party, 'bt', campaign=True, merge=True) for party in parties}\n",
    "campaign_subs_eu = {party: get_subs(party, 'eu', campaign=True, merge=True) for party in parties}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bt_sub_words = pd.Series([len(corpus_bt[party]) for party in parties], index=parties.keys())\n",
    "eu_sub_words = pd.Series([len(corpus_eu[party]) for party in parties], index=parties.keys())\n",
    "\n",
    "fig = plot_compare_bar(bt_sub_words, eu_sub_words, 'Word count')\n",
    "\n",
    "fig.savefig('outputs/yt-word-count.pdf', bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic analysis algorithms\n",
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct(df, n=10):\n",
    "    \"\"\" Takes a pandas matrix, where rows are words and columns parties, \n",
    "    and returns the n highest distinct words.\n",
    "    \"\"\"\n",
    "    matrix = df.copy()\n",
    "    parties = matrix.columns\n",
    "    entries = {party: [] for party in parties}\n",
    "    while any(map(lambda key: len(entries[key]) < n, entries)):\n",
    "        next_party = matrix.max().idxmax()\n",
    "        word = matrix[next_party].idxmax()\n",
    "        matrix.drop(word, inplace=True)\n",
    "        entries[next_party].append(word)\n",
    "        if len(entries[next_party]) >= n:\n",
    "            matrix.drop(next_party, axis=1, inplace=True)\n",
    "    return pd.DataFrame(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "v_bt = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode')\n",
    "x_bt = v_bt.fit_transform(\" \".join(corpus_bt[party]) for party in parties)\n",
    "df_bt = pd.DataFrame(x_bt.T.toarray(), index=v_bt.get_feature_names(), columns=parties)\n",
    "display_side_by_side(*(top_n(df_bt, party, 15) for party in parties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct(df_bt, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_eu = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', norm='l2')\n",
    "x_eu = v_eu.fit_transform(\" \".join(corpus_eu[party]) for party in parties)\n",
    "df_eu = pd.DataFrame(x_eu.T.toarray(), index=v_eu.get_feature_names(), columns=parties)\n",
    "display_side_by_side(*(top_n(df_eu, party, 15) for party in parties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct(df_eu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "fig = generate_wordcloud(df_bt, \"TF-IDF - 2016 Bundestag election\")\n",
    "fig.savefig('outputs/yt-tfidf-wordcloud-bt.pdf', bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_wordcloud(df_eu, \"TF-IDF - 2019 European election\")\n",
    "fig.savefig('outputs/yt-tfidf-wordcloud-eu.pdf', bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod import tm_lda\n",
    "from tmtoolkit.topicmod.visualize import plot_eval_results\n",
    "from tmtoolkit.topicmod.evaluate import results_by_parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tags'] = data['tags'].apply(lambda t: t.lower().split(\"'\")[1::2])\n",
    "\n",
    "climate_tags = ['klima', 'klimaschutz', 'umwelt', 'co2', 'klimaerwärmung', 'klimawandel']\n",
    "refugee_tags = ['flüchtling', 'einwanderung', 'migrant', 'migranten', 'ausländer', 'zuwanderung', 'flucht']\n",
    "economy_tags = ['wirtschaft', 'handelsabkommen']\n",
    "\n",
    "def get_videos_with_certain_tags(tags):\n",
    "    return data['tags'].apply(lambda vtags: any([t in vtags for t in tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_ids = get_videos_with_certain_tags(climate_tags)\n",
    "refugee_ids = get_videos_with_certain_tags(refugee_tags)\n",
    "economy_ids = get_videos_with_certain_tags(economy_tags)\n",
    "print(\"Climate: \", len(climate_ids), \" Refugee: \", len(refugee_ids), \" Economy: \", len(economy_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_corpus = data[climate_ids | refugee_ids | economy_ids]['subtitle'].apply(lambda sub: preprocess(sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_vids = TfidfVectorizer(sublinear_tf=True, \n",
    "                         strip_accents='unicode', \n",
    "                         preprocessor=lambda x: x, \n",
    "                         tokenizer=lambda x: x)\n",
    "x_vids = v_vids.fit_transform([sub for sub in tag_corpus])\n",
    "df_vids = pd.DataFrame(x_vids.T.toarray(), index=v_vids.get_feature_names(), columns=tag_corpus.index)\n",
    "tags = []\n",
    "for vid in df_vids.columns:\n",
    "    tags.append(df_vids.sort_values(vid)[-1:])\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x)\n",
    "x = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "        \n",
    "lda = LDA(n_components=10, max_iter=20)\n",
    "lda.fit(x)\n",
    "print_topics(lda, cv, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphKernel",
   "language": "python",
   "name": "graphkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
